{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTOdkhmt7zON"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMpPvXl8g9Yp"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain langsmith langchainhub langchain_community langgraph langgraph_swarm --quiet\n",
        "%pip install --upgrade --quiet  langchain-openai tiktoken pandas duckduckgo-search qdrant_client --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffn8_AL8UMIG"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A0YI98F_T5Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS7GBHPn-Pwp"
      },
      "outputs": [],
      "source": [
        "# from unsloth import FastLanguageModel\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = \"/content/drive/MyDrive/5293 final/my_lora_adapters\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "#     max_seq_length = 2048,\n",
        "#     load_in_4bit = True,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CbUCmpz-aXN"
      },
      "outputs": [],
      "source": [
        "# model.save_pretrained_merged(\"/content/drive/MyDrive/5293 final/merged_model\", tokenizer, save_method = \"merged_16bit\",)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTwMIw1oBVyP"
      },
      "source": [
        "# Load merged model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR7JtTqvBUoz"
      },
      "outputs": [],
      "source": [
        "# from unsloth import FastLanguageModel\n",
        "\n",
        "# # Load the saved model\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name=\"/content/drive/MyDrive/5293 final/merged_model\",\n",
        "#     max_seq_length=2048,\n",
        "#     dtype=None,\n",
        "#     load_in_4bit=True,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwPwrle4BZe8"
      },
      "outputs": [],
      "source": [
        "# # Test the loaded model\n",
        "# messages = [\n",
        "#     {\"role\": \"user\", \"content\": \"How to cancel order?\"}\n",
        "# ]\n",
        "\n",
        "# text = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize=False,\n",
        "#     add_generation_prompt=True,\n",
        "#     enable_thinking=False,\n",
        "# )\n",
        "\n",
        "# from transformers import TextStreamer\n",
        "# _ = model.generate(\n",
        "#     **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
        "#     max_new_tokens=2048,\n",
        "#     temperature=0.7,\n",
        "#     top_p=0.8,\n",
        "#     top_k=20,\n",
        "#     streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKMP-sPAaVPj"
      },
      "source": [
        "# Load QWQ 8B fine tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157,
          "referenced_widgets": [
            "a44621eb23304c3c9b93a2a4dee14844",
            "9959125fd3f444638691b01c11d7f84e",
            "bbc1da9391f24265bd1a562eb35e68b9",
            "128b5c78303f492e920e16b2a8ead19c",
            "75017b3b74dd47e68956c59e64327316",
            "a3603b8e7ebb4ba4b274f64cf606ad8d",
            "ea90a8f4d56e469b94a0e9962a1b0f6c",
            "436f77f9170e4ed9a5c50a62f835f7f7",
            "98d4d496b8c849bd8cd1d35d77c5b18b",
            "c21fddb5aef54e168b0ca0ffc1fbee17",
            "4440191004614f35abb3d0e4cad7b829"
          ]
        },
        "id": "em8JyzweYhec",
        "outputId": "2b21583f-0acd-4586-d30f-1d3cbf41fbba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.5.6: Fast Qwen3 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a44621eb23304c3c9b93a2a4dee14844"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load the saved model\n",
        "rules_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"/content/drive/MyDrive/5293 final/merged_model\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmKXdPpCaaNK"
      },
      "source": [
        "# Tool building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJpvGSo1qCrJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import json\n",
        "import re\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from typing import Annotated\n",
        "from langchain_core.tools import tool\n",
        "from google.colab import userdata\n",
        "from typing import List, Optional\n",
        "from langchain.vectorstores import Qdrant\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.exceptions import UnexpectedResponse\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "api_key=\"your api key\"\n",
        "working_dir = \"/kaggle/working/\"\n",
        "\n",
        "vectorDB_dir = \"/kaggle/input/vecdb-m-and-s-stat5293003finalpre/\"\n",
        "data_dir = \"/kaggle/input/h-and-m-personalized-fashion-recommendations/\"\n",
        "\n",
        "QDRANT_URL = \"https://bcff4f0d-b222-4a69-bdc3-a56d740aeb21.europe-west3-0.gcp.cloud.qdrant.io:6333\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.IoZWooHacYunQbp4q9-90eSY_vtNrO3hBlnLW_qJync\"\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_recommendations(\n",
        "    user_query: str,\n",
        "    top_k: int = 3,\n",
        "    model_name: str = \"ft:gpt-3.5-turbo-0125:personal::BWoy2I4Y\",\n",
        "    api_key: Optional[str] = api_key,\n",
        "    qdrant_path: str = working_dir,\n",
        "    collection_name: str = \"STAT5293\",\n",
        "    docs: Optional[List[str]] = None\n",
        ") -> List[str]:\n",
        "    \"\"\"Search for similar fashion items using vector similarity search.\n",
        "\n",
        "    Connects to a cloud-hosted Qdrant collection containing fashion product data\n",
        "    and returns the most similar items to the user's search query.\n",
        "\n",
        "    Args:\n",
        "        user_query: Search query describing desired fashion item (e.g., 'summer dress')\n",
        "        top_k: Number of similar items to return (default: 3)\n",
        "        model_name: Fine-tuned model name for processing\n",
        "        api_key: OpenAI API key (optional if set in environment)\n",
        "        qdrant_path: Working directory path\n",
        "        collection_name: Qdrant collection name to search\n",
        "        docs: Optional list of documents (unused)\n",
        "\n",
        "    Returns:\n",
        "        List of Document objects containing similar fashion items,\n",
        "        or \"Not Found\" if collection doesn't exist\n",
        "\n",
        "    Example:\n",
        "        results = get_fashion_recommendations('black leather jacket', top_k=5)\n",
        "    \"\"\"\n",
        "    # Set the API key\n",
        "    api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OpenAI API key must be provided either as a parameter or through the OPENAI_API_KEY environment variable.\")\n",
        "\n",
        "    # Construct the full path to the SQLite file\n",
        "    sqlite_file = os.path.join(qdrant_path, f\"{collection_name}.sqlite\")\n",
        "\n",
        "    # Initialize the embedding model\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "    # # Check and load or create the Qdrant vector store\n",
        "    QDRANT_URL = \"https://bcff4f0d-b222-4a69-bdc3-a56d740aeb21.europe-west3-0.gcp.cloud.qdrant.io:6333\"\n",
        "    QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.IoZWooHacYunQbp4q9-90eSY_vtNrO3hBlnLW_qJync\"\n",
        "\n",
        "    # loggin to cloud service\n",
        "    client = QdrantClient(\n",
        "        url=QDRANT_URL,\n",
        "        api_key=QDRANT_API_KEY,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        client.get_collection(collection_name)\n",
        "        # load existing database\n",
        "        qdrant = Qdrant.from_existing_collection(\n",
        "            embedding=embeddings,\n",
        "            path=None,\n",
        "            url=QDRANT_URL,\n",
        "            api_key=QDRANT_API_KEY,\n",
        "            collection_name=\"STAT5293\"\n",
        "        )\n",
        "\n",
        "    except UnexpectedResponse as e:\n",
        "        return \"Not Found\"\n",
        "\n",
        "    # Perform similarity search\n",
        "    results = qdrant.similarity_search(user_query, k=top_k)\n",
        "    for i, doc in enumerate(results, 1):\n",
        "        print(f\"[{i}] {doc.page_content[:300]}...\")\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "    # Build the prompt template\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"question\", \"context\"],\n",
        "        template=\"\"\"\n",
        "            You are a fashion assistant. Your task is to recommend clothing items or styles based on the user's request and the following context snippets from a fashion article database.\n",
        "\n",
        "            User query: {question}\n",
        "\n",
        "            Related fashion knowledge:\n",
        "            {context}\n",
        "\n",
        "            Now return a list of top recommended styles or items, ordered by relevance. Only list them as a bullet-point list.\n",
        "            \"\"\"\n",
        "    )\n",
        "\n",
        "    # Initialize the language model with the fine-tuned model\n",
        "    llm = ChatOpenAI(model=model_name, api_key=api_key)\n",
        "\n",
        "    # Create the chain\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "    # Run the chain and get the response\n",
        "    response = chain.run({\"question\": user_query, \"context\": context})\n",
        "\n",
        "    # Parse the response to extract the list of recommendations\n",
        "    explains = [line.strip(\"•- \").strip() for line in response.strip().split(\"\\n\") if line.strip()]\n",
        "\n",
        "    # Reranking the retrievial results\n",
        "    import re\n",
        "    from typing import List\n",
        "    from langchain.schema import Document\n",
        "\n",
        "    def extract_title_before_colon(text: str) -> str:\n",
        "        \"\"\"\n",
        "        extract the title before colon\n",
        "        \"\"\"\n",
        "        match = re.match(r'^(.*?)\\s*:', text.strip())\n",
        "        if match:\n",
        "            return match.group(1).strip().lower()  # 统一小写\n",
        "        return \"\"\n",
        "\n",
        "    def normalize_text(text: str) -> str:\n",
        "        \"\"\"\n",
        "        nomalize text for fuzzy match\n",
        "        \"\"\"\n",
        "        return re.sub(r'[^a-z0-9\\s]', '', text.lower()).strip()\n",
        "\n",
        "    def rerank_results_by_rerank_titles(results: List[Document], results_rerank: List[str]) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Rerank the results based on the order of titles in results_rerank.Capital letters are ignored.\n",
        "        \"\"\"\n",
        "        ranked_titles = [normalize_text(extract_title_before_colon(r)) for r in results_rerank]\n",
        "        ordered_results = []\n",
        "        used_doc_ids = set()\n",
        "\n",
        "        for title in ranked_titles:\n",
        "            found = False\n",
        "            for doc in results:\n",
        "                if doc.metadata['_id'] in used_doc_ids:\n",
        "                    continue\n",
        "                # normalize the content for fuzzy match\n",
        "                normalized_doc_text = normalize_text(doc.page_content)\n",
        "                if title in normalized_doc_text:\n",
        "                    ordered_results.append(doc)\n",
        "                    used_doc_ids.add(doc.metadata['_id'])\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                print(f\"Warning: Title '{title}' not found in any document.\")\n",
        "        return ordered_results\n",
        "\n",
        "    return rerank_results_by_rerank_titles(results, explains)\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"final-5293\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "\n",
        "@tool\n",
        "def retrieve_rules(prompt) -> str:\n",
        "    \"\"\"Retrieve E-commerce rule. You should pass in the query string and expect a return of a rule in string.\"\"\"\n",
        "    system_prompt = \"You are a helpful customer service representative on Amazon. Be concise and helpful.\"\n",
        "    # model.eval()\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = rules_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "# Usage:\n",
        "# response = qwen_generate(model, tokenizer, \"What is your return policy?\")\n",
        "# print(response)\n",
        "\n",
        "# Tavily Search\n",
        "search = TavilySearchResults(max_results=3, verbose=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# customer_tools = [search, retrieve_order_status, retrieve_rules, retrieve_purchase_history]\n",
        "# recommendation_tools = [retrieve_recommendation, retrieve_purchase_history]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tfd1o5VnSjr3"
      },
      "outputs": [],
      "source": [
        "retrieve_rules.invoke(\"My order is not shipped yet, can I cancel it?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QOi11AsacVl"
      },
      "source": [
        "# Agent building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuPMzT9JV10w"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph_swarm import create_handoff_tool, create_swarm\n",
        "from langgraph.checkpoint.memory import InMemorySaver, MemorySaver\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
        "from datetime import datetime\n",
        "db = SQLDatabase.from_uri(\"sqlite:////content/drive/MyDrive/5293 final/mydatabase.db\")\n",
        "model = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# Create handoff tools\n",
        "handoff_to_recommender = create_handoff_tool(\n",
        "    agent_name=\"Recommender_agent\",\n",
        "    description=\"Transfer to Recommender_agent for product suggestions and recommendations.\"\n",
        ")\n",
        "\n",
        "handoff_to_customer_service = create_handoff_tool(\n",
        "    agent_name=\"Customer_Service_Agent\",\n",
        "    description=\"Transfer to Customer_Service_Agent for policies, returns, and general support.\"\n",
        ")\n",
        "\n",
        "handoff_to_sql = create_handoff_tool(\n",
        "    agent_name=\"SQL_agent\",\n",
        "    description=\"Transfer to SQL_agent for database queries on inventory, orders, and shipping.\"\n",
        ")\n",
        "\n",
        "# Create customer service tools (assuming these exist)\n",
        "customer_service_tools = [retrieve_rules] + [handoff_to_recommender, handoff_to_sql]\n",
        "\n",
        "# Create recommendation tools (assuming these exist)\n",
        "recommendation_tools = [search, get_recommendations] + [handoff_to_customer_service, handoff_to_sql]\n",
        "\n",
        "# Create sql tools (assuming these exist)\n",
        "sql_tools = SQLDatabaseToolkit(db=db, llm=model).get_tools() + [handoff_to_customer_service, handoff_to_recommender]\n",
        "\n",
        "\n",
        "# Create agents\n",
        "Customer_Service_Agent = create_react_agent(\n",
        "    model,\n",
        "    tools=customer_service_tools,\n",
        "    name=\"Customer_Service_Agent\",\n",
        "    prompt=\"\"\"You are an Amazon helpful customer service representative.\n",
        "    If it is policy-related, first retrieve the rule then provide solutions or replies based on the rules and user's information.\"\"\"\n",
        ")\n",
        "\n",
        "Recommender_agent = create_react_agent(\n",
        "    model,\n",
        "    tools=recommendation_tools,\n",
        "    name=\"Recommender_agent\",\n",
        "    prompt=f\"\"\"You are a product recommendation specialist.\n",
        "    Current date and time: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "    Recommend based on following procedure:\n",
        "    1. User's order history\n",
        "    2. Current trend in the internet -> search\n",
        "    3. What we have in stock -> get_recommendations\n",
        "    For query that is not in your scope, handoff it to the corresponding agents.\n",
        "    If in-stock items do not match, fall back to current trend in the internet to deliver recommendations.\n",
        "    Provide specific product suggestions with brief explanations.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are an Amazon helpful database manager that interact with a SQL database.\n",
        "Given an input question, create a syntactically correct {dialect} query to run,\n",
        "then look at the results of the query and return the answer. Unless the user\n",
        "specifies a specific number of examples they wish to obtain, always limit your\n",
        "query to at most {top_k} results.\n",
        "\n",
        "You can order the results by a relevant column to return the most interesting\n",
        "examples in the database. Never query for all the columns from a specific table,\n",
        "only ask for the relevant columns given the question.\n",
        "\n",
        "You MUST double check your query before executing it. If you get an error while\n",
        "executing a query, rewrite the query and try again.\n",
        "\n",
        "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\n",
        "database.\n",
        "\n",
        "To start you should ALWAYS look at the tables in the database to see what you\n",
        "can query. Do NOT skip this step.\n",
        "\n",
        "Then you should query the schema of the most relevant tables.\n",
        "\"\"\".format(\n",
        "    dialect=db.dialect,\n",
        "    top_k=5,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "SQL_agent = create_react_agent(\n",
        "    # \"openai:gpt-4o\",    # <-- model spec string, not a ChatOpenAI object\n",
        "    model,\n",
        "    sql_tools,\n",
        "    name = \"SQL_agent\",\n",
        "    prompt=system_prompt,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "\n",
        "builder = create_swarm(\n",
        "    [Customer_Service_Agent, Recommender_agent, SQL_agent],\n",
        "    default_active_agent=\"Customer_Service_Agent\"\n",
        ")\n",
        "\n",
        "\n",
        "app = builder.compile(checkpointer=checkpointer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jBIlsFsENiG3"
      },
      "outputs": [],
      "source": [
        "!pip install grandalf --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEbU98Cjmn7M"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "# display(Image(app.get_graph().draw_mermaid_png()))\n",
        "app.get_graph().print_ascii()\n",
        "# app.get_graph()\n",
        "# You can also render it in a markdown cell by copying the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s5gOieUsvyK"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4()), \"user_id\": \"1\"}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROyHklR1y2x5"
      },
      "outputs": [],
      "source": [
        "def print_stream(stream):\n",
        "    for ns, update in stream:\n",
        "        print(f\"Namespace '{ns}'\")\n",
        "        for node, node_updates in update.items():\n",
        "            if node_updates is None:\n",
        "                continue\n",
        "\n",
        "            if isinstance(node_updates, (dict, tuple)):\n",
        "                node_updates_list = [node_updates]\n",
        "            elif isinstance(node_updates, list):\n",
        "                node_updates_list = node_updates\n",
        "            else:\n",
        "                raise ValueError(node_updates)\n",
        "\n",
        "            for node_updates in node_updates_list:\n",
        "                print(f\"Update from node '{node}'\")\n",
        "                if isinstance(node_updates, tuple):\n",
        "                    print(node_updates)\n",
        "                    continue\n",
        "                messages_key = next(\n",
        "                    (k for k in node_updates.keys() if \"messages\" in k), None\n",
        "                )\n",
        "                if messages_key is not None:\n",
        "                    node_updates[messages_key][-1].pretty_print()\n",
        "                else:\n",
        "                    print(node_updates)\n",
        "\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "    print(\"\\n===\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD8FPrA6y4t9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tracers.context import tracing_v2_enabled\n",
        "\n",
        "\n",
        "with tracing_v2_enabled(project_name= \"5293\"):\n",
        "  print_stream(\n",
        "      app.stream(\n",
        "          {\n",
        "              \"messages\": [\n",
        "                  {\n",
        "                      \"role\": \"user\",\n",
        "                      \"content\": \"User id:e481f51cbdc54678b7cc49136f2d6af7.Recommend me something based on what I bought?\",\n",
        "                  }\n",
        "              ]\n",
        "          },\n",
        "          config,\n",
        "          subgraphs=True,\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF50HDhCdCbj"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from langchain_core.tracers.context import tracing_v2_enabled\n",
        "\n",
        "# Stream processor (same as original)\n",
        "def print_stream(stream):\n",
        "    for ns, update in stream:\n",
        "        print(f\"Namespace '{ns}'\")\n",
        "        for node, node_updates in update.items():\n",
        "            if node_updates is None: continue\n",
        "            updates_list = [node_updates] if isinstance(node_updates, (dict, tuple)) else node_updates\n",
        "            for updates in updates_list:\n",
        "                print(f\"Update from node '{node}'\")\n",
        "                if isinstance(updates, tuple):\n",
        "                    print(updates)\n",
        "                    continue\n",
        "                messages_key = next((k for k in updates.keys() if \"messages\" in k), None)\n",
        "                if messages_key:\n",
        "                    updates[messages_key][-1].pretty_print()\n",
        "                else:\n",
        "                    print(updates)\n",
        "        print(\"\\n\\n\")\n",
        "    print(\"\\n===\\n\")\n",
        "\n",
        "# UI components\n",
        "output = widgets.Output(layout=widgets.Layout(border='1px solid gray', padding='10px', height='400px', overflow='auto'))\n",
        "text_input = widgets.Text(placeholder=\"Enter your message...\", layout=widgets.Layout(width='80%'))\n",
        "send_btn = widgets.Button(description=\"Send\", button_style='primary')\n",
        "# Send handler\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4()), \"user_id\": \"1\"}}\n",
        "def on_send(b):\n",
        "    message = text_input.value.strip()\n",
        "    if not message: return\n",
        "\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        with tracing_v2_enabled(project_name=\"5293\"):\n",
        "            print_stream(app.stream({\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": message}]\n",
        "            }, config, subgraphs=True))\n",
        "    text_input.value = \"\"\n",
        "\n",
        "\n",
        "\n",
        "send_btn.on_click(on_send)\n",
        "\n",
        "# Display\n",
        "ui = widgets.VBox([output, widgets.HBox([text_input, send_btn])])\n",
        "display(ui)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a44621eb23304c3c9b93a2a4dee14844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9959125fd3f444638691b01c11d7f84e",
              "IPY_MODEL_bbc1da9391f24265bd1a562eb35e68b9",
              "IPY_MODEL_128b5c78303f492e920e16b2a8ead19c"
            ],
            "layout": "IPY_MODEL_75017b3b74dd47e68956c59e64327316"
          }
        },
        "9959125fd3f444638691b01c11d7f84e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3603b8e7ebb4ba4b274f64cf606ad8d",
            "placeholder": "​",
            "style": "IPY_MODEL_ea90a8f4d56e469b94a0e9962a1b0f6c",
            "value": "Loading checkpoint shards:  25%"
          }
        },
        "bbc1da9391f24265bd1a562eb35e68b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_436f77f9170e4ed9a5c50a62f835f7f7",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98d4d496b8c849bd8cd1d35d77c5b18b",
            "value": 1
          }
        },
        "128b5c78303f492e920e16b2a8ead19c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c21fddb5aef54e168b0ca0ffc1fbee17",
            "placeholder": "​",
            "style": "IPY_MODEL_4440191004614f35abb3d0e4cad7b829",
            "value": " 1/4 [01:42&lt;05:08, 102.87s/it]"
          }
        },
        "75017b3b74dd47e68956c59e64327316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3603b8e7ebb4ba4b274f64cf606ad8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea90a8f4d56e469b94a0e9962a1b0f6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "436f77f9170e4ed9a5c50a62f835f7f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98d4d496b8c849bd8cd1d35d77c5b18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c21fddb5aef54e168b0ca0ffc1fbee17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4440191004614f35abb3d0e4cad7b829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}